{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b05cfc-bf19-468b-9b55-02f0aa40f7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import  numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e26b9d7-af98-445c-b71e-2799662a1712",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "data_path=os.getenv(\"data_path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e61464-76fc-4c98-ae46-1db81ef31854",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Steps in the code\n",
    "\n",
    "1. Decision Making Function (decision_maker):\n",
    "\n",
    "Once the 100 row (100 triggers tested for scores) the selected trigger is decided in the function. Selects DataFrame rows based on multiple criteria involving hanssen_kuipers_scores, bias_scores, and auroc_scores, returning rows with the maximum trigger value. This function is DEVELOPMENT. The current iteration of decision is as follows\n",
    "\n",
    "   Steps:\n",
    "1. Filters rows with the maximum hanssen_kuipers_scores and auroc_scores above 0.5.\n",
    "2. Further filters by trigger_values greater than 0.1 and bias_scores below 1.0, selecting those with the maximum bias_score and auroc_scores above 0.5.\n",
    "3. Additionally, selects rows with the maximum heidke_skill_scores and auroc_scores above 0.5.\n",
    "4. Finally, from the combined filtered set, selects rows with the maximum trigger_values.\n",
    "\n",
    "\n",
    "2. Environment Setup:\n",
    "\n",
    "Imports necessary libraries (os, pandas, glob, numpy).\n",
    "Loads environment variables using dotenv, specifically fetching the data_path.\n",
    "\n",
    "3. Mapping Function (map_cell):\n",
    "\n",
    "Splits a given cell value by _ and maps the first part to a corresponding value using a predefined dictionary.\n",
    "\n",
    "4. Replacement Function (replace_with_list):\n",
    "\n",
    "Replaces NaN float values with a predefined list of replacement values, otherwise returns the value unchanged.\n",
    "\n",
    "\n",
    "5. Rounding Function (round_list):\n",
    "\n",
    "Rounds each element in a list to a specified number of decimal places.\n",
    "\n",
    "6. Data Processing:\n",
    "\n",
    "Iterates through CSV files in the specified data path, applying decision_maker to filter and modify each DataFrame.\n",
    "Merges the modified DataFrames, applies the map_cell function, and merges additional information from another CSV.\n",
    "\n",
    "7. Pivoting and Final Adjustments:\n",
    "\n",
    "Performs pivot operations to restructure the data for easier analysis and visualization.\n",
    "Applies the replacement function to DataFrame cells and rounds off values as needed.\n",
    "\n",
    "8. Exporting Results:\n",
    "\n",
    "Saves the final processed and pivoted DataFrame to CSV files, one for POD (Probability of Detection) values and another for FAR (False Alarm Ratio).\n",
    "\n",
    "10. Custom Mapping and Merging:\n",
    "\n",
    "Utilizes a mapping_dict to associate cell values to regions and merges additional column information based on a unique identifier.\n",
    "\n",
    "11. Final Data Preparation and Saving:\n",
    "\n",
    "Prepares the data with specific columns, applies custom functions for value adjustments, and exports the final pivoted DataFrame to CSV files, indicating performance metrics for different regions and seasons. The resultant pod.csv and far.csv files are read and made into table plots in 06-plot-table. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af9d964-71f9-40a7-9db7-f3d42530336f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def map_cell(cell_value):\n",
    "    \"\"\"\n",
    "    Maps a cell value to a corresponding value using a predefined dictionary based on the first part of the cell value.\n",
    "\n",
    "    Parameters:\n",
    "    - cell_value (str): The value of the cell to be mapped, expected to contain parts separated by '_'.\n",
    "\n",
    "    Returns:\n",
    "    - The mapped value if the first part of `cell_value` is an integer key present in `mapping_dict`. Otherwise, returns None.\n",
    "\n",
    "    Note:\n",
    "    - This function relies on a global variable `mapping_dict` which should be a dictionary where keys are integers.\n",
    "    \"\"\"\n",
    "    cell_parts = cell_value.split('_')\n",
    "    if len(cell_parts) > 1:\n",
    "        key = int(cell_parts[0])\n",
    "        if key in mapping_dict:\n",
    "            return mapping_dict[key]\n",
    "    return None\n",
    "\n",
    "\n",
    "def replace_with_list(x):\n",
    "    \"\"\"\n",
    "    Replaces NaN float values with a predefined list of replacement values.\n",
    "\n",
    "    Parameters:\n",
    "    - x (float): The input value to be checked and potentially replaced.\n",
    "\n",
    "    Returns:\n",
    "    - A list of replacement values if `x` is a float and is NaN. Otherwise, returns `x` unchanged.\n",
    "\n",
    "    Note:\n",
    "    - This function is designed to handle cases where cell values in a dataset need to be replaced with a list of values\n",
    "      for indicating missing or special cases.\n",
    "    \"\"\"\n",
    "    replacement_values = [-999.0, -999.0]\n",
    "    # If x is a float and it is nan (meaning the cell was originally empty), return the replacement list\n",
    "    if isinstance(x, float) and np.isnan(x):\n",
    "        return replacement_values\n",
    "    # Otherwise, return x as it is\n",
    "    return x\n",
    "\n",
    "\n",
    "def decision_maker(df):\n",
    "     \"\"\"\n",
    "    Selects rows from a DataFrame based on multiple criteria related to hanssen_kuipers_scores, bias_scores, and auroc_scores.\n",
    "\n",
    "    The function performs several filtering steps to select rows that represent optimal conditions based on the specified metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame containing score metrics.\n",
    "\n",
    "    Returns:\n",
    "    - tv_max (pandas.DataFrame): A DataFrame containing the rows that match the final criteria, particularly those with the maximum trigger value.\n",
    "\n",
    "    Steps:\n",
    "    1. Filters rows with the maximum hanssen_kuipers_scores and auroc_scores above 0.5.\n",
    "    2. Further filters by trigger_values greater than 0.1 and bias_scores below 1.0, selecting those with the maximum bias_score and auroc_scores above 0.5.\n",
    "    3. Additionally, selects rows with the maximum heidke_skill_scores and auroc_scores above 0.5.\n",
    "    4. Finally, from the combined filtered set, selects rows with the maximum trigger_values.\n",
    "    \"\"\"\n",
    "    hanssen_max_rows = df[(df['hanssen_kuipers_scores'] == df['hanssen_kuipers_scores'].max()) & (df['auroc_scores'] > 0.5)]\n",
    "    df_filtered = df[df['trigger_values'] > 0.1]\n",
    "    # Step 2: From those, further filter where bias score is below 1.0\n",
    "    df_filtered = df_filtered[df_filtered['bias_scores'] < 1.0]\n",
    "    # # Step 3: Now, select rows with the maximum bias score that also have auroc_scores above 0.5\n",
    "    max_bias_below_one = df_filtered['bias_scores'].max()\n",
    "    final_filtered_df = df_filtered[(df_filtered['bias_scores'] == max_bias_below_one) & (df_filtered['auroc_scores'] > 0.5)]\n",
    "    heidke_max_rows = df[(df['heidke_skill_scores'] == df['heidke_skill_scores'].max()) & (df['auroc_scores'] > 0.5)]\n",
    "    edf=pd.concat([hanssen_max_rows, final_filtered_df, heidke_max_rows])\n",
    "    tv_max=edf[(edf['trigger_values'] == edf['trigger_values'].max())]\n",
    "    #tv_max.to_csv(f'{output_path}{region_id}_{season_str}_{level}_lt{lead_int}.csv')\n",
    "    return tv_max\n",
    "\n",
    "\n",
    "def round_list(lst, decimal_places):\n",
    "    \"\"\"\n",
    "    Rounds each element in a list to a specified number of decimal places.\n",
    "\n",
    "    Parameters:\n",
    "    - lst (list of float): The list of numbers to be rounded.\n",
    "    - decimal_places (int): The number of decimal places to round each number to.\n",
    "\n",
    "    Returns:\n",
    "    - A list containing the rounded values of the input list.\n",
    "\n",
    "    Note:\n",
    "    - This function is useful for rounding numerical values in a list to ensure consistency or to improve readability.\n",
    "    \"\"\"\n",
    "    return [round(x, decimal_places) for x in lst]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4e325b-3323-433e-9782-7f85bf6ffe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict = {0: 'Karamoja', 1: 'Marsabit', 2: 'Wajir'}\n",
    "\n",
    "# List of CSV files\n",
    "csv_files = glob.glob(f'{data_path}*.csv')\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Read each CSV file into a DataFrame, add a new column with the file name, and append to dfs list\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    ddf=decision_maker(df)\n",
    "    # Extract the file name without extension\n",
    "    filename = os.path.splitext(os.path.basename(file))[0]\n",
    "    # Add a new column with the file name\n",
    "    ddf['filename'] = filename\n",
    "    # Append the modified DataFrame to dfs list\n",
    "    dfs.append(ddf)\n",
    "\n",
    "# Concatenate all DataFrames row-wise\n",
    "db = pd.concat(dfs, ignore_index=True)\n",
    "db1=db.drop_duplicates('filename')\n",
    "\n",
    "# Apply the mapping function to create a new column\n",
    "db1['region_x'] = db1['filename'].apply(map_cell)\n",
    "\n",
    "\n",
    "df1=pd.read_csv(f'{data_path}code_names.csv')\n",
    "\n",
    "\n",
    "# Create a unique identifier in df1\n",
    "df1['identifier'] = df1['region_id'].astype(str) + '_'+df1['season'] + '_' + df1['level'] + '_' + df1['lt']\n",
    "\n",
    "db1['identifier'] = db1['filename']\n",
    "db2 = db1.merge(df1[['identifier', 'col_name']], on='identifier', how='left')\n",
    "db2['spi_prod_x'] = db2['filename'].str.split('_').str[1]\n",
    "db3=db2[['trigger_values','hit_rates','false_alarm_ratios','region_x','col_name','spi_prod_x']]\n",
    "db3['trigger_values']=db3['trigger_values']*100\n",
    "db3['values'] = db3.apply(lambda x: [x['hit_rates'], x['trigger_values']], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "db3['values'] = db3['values'].apply(lambda x: round_list(x, 2))\n",
    "\n",
    "\n",
    "pivoted_df = db3.pivot_table(index=['region_x', 'spi_prod_x'], \n",
    "                            columns='col_name', \n",
    "                            values='values', \n",
    "                            aggfunc='first')  # Assuming each combination is unique\n",
    "\n",
    "# Reset the index to turn it back into columns\n",
    "pivoted_df = pivoted_df.reset_index()\n",
    "\n",
    "\n",
    "# Apply the custom function to each cell in the DataFrame\n",
    "pivoted_df = pivoted_df.applymap(replace_with_list)\n",
    "\n",
    "pivoted_df.columns.name = None\n",
    "\n",
    "pivoted_df['empty1']=[[-999.0, -999.0]]* len(pivoted_df)\n",
    "pivoted_df['empty2']=[[-999.0, -999.0]]* len(pivoted_df)\n",
    "\n",
    "\n",
    "#efg_col=['region_x', 'spi_prod_x', 'nov_x', 'dec_x', 'jan_x', 'feb_x', 'mar_x', 'jun_x', 'jul_x', 'aug_x', 'sep_x', 'oct_x', 'nov_y', 'dec_y', 'jan_y', 'feb_y', 'mar_y', 'jun_y', 'jul_y', 'aug_y', 'sep_y', 'oct_y', 'nov', 'dec', 'jan', 'feb', 'mar', 'jun', 'jul', 'aug', 'sep', 'oct']\n",
    "\n",
    "efg_col1=['region_x', 'spi_prod_x', 'nov_x', 'dec_x', 'jan_x',\n",
    "       'feb_x', 'mar_x', 'apr_x', 'may_x', 'jun_x', 'jul_x', 'aug_x', 'sep_x',\n",
    "       'oct_x', 'empty1', 'nov_y', 'dec_y', 'jan_y', 'feb_y', 'mar_y', 'apr_y',\n",
    "       'may_y', 'jun_y', 'jul_y', 'aug_y', 'sep_y', 'oct_y', 'empty2', 'nov',\n",
    "       'dec', 'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep',\n",
    "       'oct']\n",
    "pivoted_df1=pivoted_df[efg_col1]\n",
    "\n",
    "\n",
    "pivoted_df1.to_csv(f'{data_path}pod.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e992207a-ba91-4057-9ca1-d8f3e368456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db3['values'] = db3.apply(lambda x: [x['false_alarm_ratios'], x['trigger_values']], axis=1)\n",
    "\n",
    "def round_list(lst, decimal_places):\n",
    "    return [round(x, decimal_places) for x in lst]\n",
    "\n",
    "db3['values'] = db3['values'].apply(lambda x: round_list(x, 2))\n",
    "\n",
    "\n",
    "pivoted_df = db3.pivot_table(index=['region_x', 'spi_prod_x'], \n",
    "                            columns='col_name', \n",
    "                            values='values', \n",
    "                            aggfunc='first')  # Assuming each combination is unique\n",
    "\n",
    "# Reset the index to turn it back into columns\n",
    "pivoted_df = pivoted_df.reset_index()\n",
    "\n",
    "\n",
    "# Apply the custom function to each cell in the DataFrame\n",
    "pivoted_df = pivoted_df.applymap(replace_with_list)\n",
    "\n",
    "pivoted_df.columns.name = None\n",
    "\n",
    "\n",
    "pivoted_df['empty1']=[[-999.0, -999.0]]* len(pivoted_df)\n",
    "pivoted_df['empty2']=[[-999.0, -999.0]]* len(pivoted_df)\n",
    "\n",
    "\n",
    "#efg_col=['region_x', 'spi_prod_x', 'nov_x', 'dec_x', 'jan_x', 'feb_x', 'mar_x', 'jun_x', 'jul_x', 'aug_x', 'sep_x', 'oct_x', 'nov_y', 'dec_y', 'jan_y', 'feb_y', 'mar_y', 'jun_y', 'jul_y', 'aug_y', 'sep_y', 'oct_y', 'nov', 'dec', 'jan', 'feb', 'mar', 'jun', 'jul', 'aug', 'sep', 'oct']\n",
    "\n",
    "efg_col1=['region_x', 'spi_prod_x', 'nov_x', 'dec_x', 'jan_x',\n",
    "       'feb_x', 'mar_x', 'apr_x', 'may_x', 'jun_x', 'jul_x', 'aug_x', 'sep_x',\n",
    "       'oct_x', 'empty1', 'nov_y', 'dec_y', 'jan_y', 'feb_y', 'mar_y', 'apr_y',\n",
    "       'may_y', 'jun_y', 'jul_y', 'aug_y', 'sep_y', 'oct_y', 'empty2', 'nov',\n",
    "       'dec', 'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep',\n",
    "       'oct']\n",
    "pivoted_df1=pivoted_df[efg_col1]\n",
    "\n",
    "\n",
    "pivoted_df1.to_csv(f'{data_path}far.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
