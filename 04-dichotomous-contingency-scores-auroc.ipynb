{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650c1417-f153-414c-b403-fae1fc395aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import climpred\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regionmask\n",
    "import geopandas as gp\n",
    "from climpred import HindcastEnsemble\n",
    "from datetime import datetime\n",
    "\n",
    "import xhistogram.xarray as xhist\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "import xskillscore as xs\n",
    "from xbootstrap import block_bootstrap\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c173b73-a53c-47a9-bcfc-51e0c09be6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "data_path=os.getenv(\"data_path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300aea11-7745-48a3-a2b6-4790f10965c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The Forecast Evaluation Process\n",
    "\n",
    "Following process is invovled in assessing the accuracy of drought forecasts relative to observed data. \n",
    "\n",
    "The code uses contignecy function of [xskillscore](https://xskillscore.readthedocs.io/en/stable/api.html#dichotomous-only-yes-no-metrics) for Dichotomous-Only (yes/no) Metrics and [roc function](https://xskillscore.readthedocs.io/en/stable/api/xskillscore.roc.html#xskillscore.roc) of for Multi-Category Metrics ROC and AUROC.\n",
    "\n",
    "1. **Observation and Forecast Data**: The process begins with two primary datasets - observed data (`obs_data`) and ensemble forecast data (`ens_data`).\n",
    "\n",
    "2. **Threshold Determination**: A predefined threshold is used to classify whether a drought event occurred (based on observed and forecasted data). This is the threshold value for different season as discussed in the paper. The used threshold values are indicated in the function `get_threshold`.\n",
    "\n",
    "3. **Dichotomous Events Creation**:\n",
    "   - **Observed Events**: A bin edges as described in the documentation for observed events is passed taking the extent of SPI values -4 to 4. The less than threshold value is NOT following Gabriela et.al 2023 as it is following the logic of xskillscore. \n",
    "   - **Forecasted Events**: Instead of averaging ensemble forecasts, an empirical probability is calculated. This reflects the proportion of ensemble members predicting values at or below the given threshold. \n",
    "\n",
    "4. **Probability Threshold for Forecasts**: The forecast data is then classified into dichotomous events based on a trigger value. This step converts the forecast probability into a binary outcome - whether a drought is forecasted to occur or not. In the process, all the possible trigger value between 0 to 1 is subject to testing as in the line `trigger_values = xr.DataArray(np.linspace(0, 1, num=100), dims=['trigger_value'])`. All combination of probablity value is tested by using a for loop and passing the bin edges as needed by the xskillscore. the function get_triggers_bin_edges creates the bin_edges as reqeuierd by the xskillscore. \n",
    "\n",
    "5. The **Contingency Table Construction** and **Skill Score Calculation** are created following the internal logic of xskillscore which uses `xhist.histogram`. The 2D histogram (contingency table) is created from the binary observed and forecasted events. This table quantifies the relationship between observed occurrences/non-occurrences and forecasted occurrences/non-occurrences of drought. From the contingency table, various skill scores are calculated, including hit rates, false alarm ratios, and others. These metrics provide a comprehensive view of forecast accuracy and reliability.\n",
    "\n",
    "7. **AUROC Calculation**: The Area Under the Receiver Operating Characteristic (AUROC) score is calculated, offering a measure of the forecast's ability to discriminate between the occurrence and non-occurrence of drought events. Bootstrap method based on library [xbootstrap](https://pypi.org/project/xbootstrap/) and applied in xskillscore as [following the gist](https://gist.github.com/aaronspring/471e70f787aef6689825182e794421fb) was used, it uses dask and thus computationally fast. \n",
    "\n",
    "8. **Final Output**: The process culminates in the generation of a DataFrame summarizing the calculated skill scores for various trigger values, which is then saved to a CSV file. This summary facilitates the evaluation of forecast performance across different probability thresholds.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## The summary of steps are as follows\n",
    "\n",
    "1. **Load Datasets**: Based on the provided `season_str` length, either SPI3 or SPI4 datasets are loaded for both forecast (`kn_fct`) and observed (`kn_obs`) data from a specified `data_path`.\n",
    "\n",
    "2. **Generate Region Masks**: Utilizes `ken_mask_creator()` to create masks for specified regions or districts using the `regionmask` library, facilitating region-specific analyses.\n",
    "\n",
    "3. **Select Region**: Extracts geographical bounds from a combined GeoDataFrame and selects forecast and observation data within these bounds for the specified `region_id`.\n",
    "\n",
    "4. **Initialize Hindcast Ensemble**: Creates a `HindcastEnsemble` object with the selected forecast data and adds the corresponding observations to it.\n",
    "\n",
    "5. **Subset for Lead Time**: Subsets the forecast data for the given `lead_int`, ensuring analysis is conducted at the specified forecast lead time.\n",
    "\n",
    "6. **Generate Seasonal Product Names**: Depending on `season_str` length, either `spi3_prod_name_creator` or `spi4_prod_`name_creator` is called to generate a list of seasonal product names for both forecast and observed data, facilitating season-specific filtering.\n",
    "\n",
    "7. **Assign and Filter by Season**: Assigns generated seasonal product names as coordinates to both datasets and filters them to include only data corresponding to the specified `season_str`.\n",
    "\n",
    "8. **Align Time Coordinates**: Aligns the observed dataset time coordinates with the forecast dataset's valid time coordinates, ensuring that both datasets are comparable in time for analysis.\n",
    "\n",
    "\n",
    "This process ensures that the observed and forecasted datasets are correctly prepared and aligned for a specified region, season, and lead time, allowing for the accurate calculation of verification scores such as hit rates, false alarm ratios, and AUROC scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97eb6c3-359f-4e75-9383-d7d20dac6a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ken_mask_creator():\n",
    "    \"\"\"\n",
    "    Utiliity for generating region/district masks using regionmask library\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    the_mask : TYPE\n",
    "        DESCRIPTION.\n",
    "    rl_dict : TYPE\n",
    "        DESCRIPTION.\n",
    "\n",
    "    \"\"\"\n",
    "    dis=gp.read_file(f'{data_path}Karamoja_boundary_dissolved.shp')\n",
    "    mbt_path=os.getenv(\"mbt_path\")\n",
    "    reg=gp.read_file(f'{data_path}wajir_mbt_extent.shp')\n",
    "    mds=pd.concat([dis,reg])\n",
    "    mds1=mds.reset_index()\n",
    "    mds1['region']=[0,1,2]\n",
    "    mds1['region_name']=['Karamoja', 'Marsabit','Wajir']\n",
    "    mds2=mds1[['geometry','region','region_name']]\n",
    "    rl_dict=dict(zip(mds2.region, mds2.region_name))\n",
    "    the_mask = regionmask.from_geopandas(mds2,numbers='region',overlap=True)\n",
    "    return the_mask, rl_dict, mds2\n",
    "\n",
    "def spi3_prod_name_creator(ds_ens,var_name):\n",
    "    \"\"\"\n",
    "    Convenience function to generate a list of SPI product\n",
    "    names, such as MAM, so that can be used to filter the \n",
    "    SPI product from dataframe\n",
    "\n",
    "    added with method to convert the valid_time in CF format into datetime at\n",
    "    line 3, which is the format given by climpred valid_time calculation \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds_ens : xarray dataframe\n",
    "        The data farme with SPI output organized for \n",
    "        the period 1981-2023.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    spi_prod_list : String list\n",
    "        List of names with iteration of SPI3 product names such as\n",
    "        ['JFM','FMA','MAM',......]\n",
    "\n",
    "    \"\"\"\n",
    "    db=pd.DataFrame()\n",
    "    db['dt']=ds_ens[var_name].values\n",
    "    db['dt1'] = db['dt'].apply(lambda x: datetime(x.year, x.month, x.day,\n",
    "                                                                     x.hour, x.minute, x.second))\n",
    "    #db['dt1']=db['dt'].to_datetimeindex()\n",
    "    db['month']=db['dt1'].dt.strftime('%b').astype(str).str[0]\n",
    "    db['year']=db['dt1'].dt.strftime('%Y')\n",
    "    db['spi_prod'] = db.groupby('year')['month'].shift(2)+db.groupby('year')['month'].shift(1) + db.groupby('year')['month'].shift(0)\n",
    "    spi_prod_list=db['spi_prod'].tolist()\n",
    "    return spi_prod_list\n",
    "\n",
    "\n",
    "def spi4_prod_name_creator(ds_ens,var_name):\n",
    "    \"\"\"\n",
    "    Convenience function to generate a list of SPI product\n",
    "    names, such as MAM, so that can be used to filter the \n",
    "    SPI product from dataframe\n",
    "\n",
    "    added with method to convert the valid_time in CF format into datetime at\n",
    "    line 3, which is the format given by climpred valid_time calculation \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds_ens : xarray dataframe\n",
    "        The data farme with SPI output organized for \n",
    "        the period 1981-2023.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    spi_prod_list : String list\n",
    "        List of names with iteration of SPI3 product names such as\n",
    "        ['JFM','FMA','MAM',......]\n",
    "\n",
    "    \"\"\"\n",
    "    db=pd.DataFrame()\n",
    "    db['dt']=ds_ens[var_name].values\n",
    "    db['dt1'] = db['dt'].apply(lambda x: datetime(x.year, x.month, x.day,\n",
    "                                                                     x.hour, x.minute, x.second))\n",
    "    #db['dt1']=db['dt'].to_datetimeindex()\n",
    "    db['month']=db['dt1'].dt.strftime('%b').astype(str).str[0]\n",
    "    db['year']=db['dt1'].dt.strftime('%Y')\n",
    "    db['spi_prod'] = db.groupby('year')['month'].shift(3)+db.groupby('year')['month'].shift(2)+db.groupby('year')['month'].shift(1) + db.groupby('year')['month'].shift(0)\n",
    "    spi_prod_list=db['spi_prod'].tolist()\n",
    "    return spi_prod_list\n",
    "\n",
    "\n",
    "def make_obs_fct_dataset(region_id,season_str,lead_int):\n",
    "    \"\"\"\n",
    "    Prepares observed and forecasted dataset subsets for a specific region, season, and lead time.\n",
    "\n",
    "    This function loads observed and forecasted datasets based on the season string length (indicating SPI3 or SPI4),\n",
    "    applies regional masking, selects the data for the given region by its ID, and subsets the data for the specified\n",
    "    season and lead time. It then aligns the observed dataset time coordinates with the forecasted dataset valid time\n",
    "    coordinates and returns both datasets.\n",
    "\n",
    "    Parameters:\n",
    "    - region_id (int): The identifier for the region of interest.\n",
    "    - season_str (str): A string representing the season. The length of this string determines whether SPI3 or SPI4\n",
    "                        datasets are used ('mam', 'jjas', etc. for SPI3, and longer strings for SPI4).\n",
    "    - lead_int (int): The lead time index for which the forecast dataset is to be subset.\n",
    "\n",
    "    Returns:\n",
    "    - obs_data (xarray.DataArray): The subsetted observed data array for the specified region, season, and aligned time coordinates.\n",
    "    - ens_data (xarray.DataArray): The subsetted forecast data array for the specified region, season, lead time, and aligned time coordinates.\n",
    "\n",
    "    Notes:\n",
    "    - The function assumes the existence of a `data_path` variable that specifies the base path to the dataset files.\n",
    "    - It requires the `xarray` library for data manipulation and assumes specific naming conventions for the dataset files.\n",
    "    - Regional masking and season-specific processing rely on externally defined functions and naming conventions.\n",
    "    - The final alignment of observed dataset time coordinates with forecasted dataset valid time coordinates ensures\n",
    "      comparability between observed and forecasted values for verification purposes.\n",
    "\n",
    "    Example Usage:\n",
    "    >>> obs_data, ens_data = make_obs_fct_dataset(1, 'mam', 0)\n",
    "    >>> print(obs_data)\n",
    "    >>> print(ens_data)\n",
    "\n",
    "    This would load the observed and forecasted SPI3 datasets for region 1 during the 'mam' season and subset them\n",
    "    for lead time index 0, aligning the observed data time coordinates with the forecasted data valid time coordinates.\n",
    "    \"\"\"\n",
    "    if len(season_str) == 3:\n",
    "        kn_fct=xr.open_dataset(f'{data_path}kn_fct_spi3.nc')\n",
    "        kn_obs=xr.open_dataset(f'{data_path}kn_obs_spi3.nc')\n",
    "    else:\n",
    "        kn_fct=xr.open_dataset(f'{data_path}kn_fct_spi4.nc')\n",
    "        kn_obs=xr.open_dataset(f'{data_path}kn_obs_spi4.nc')\n",
    "    the_mask, rl_dict,mds1=ken_mask_creator()\n",
    "    bounds = mds1.bounds\n",
    "    #bounds.iloc[0].minx\n",
    "    llon=bounds.iloc[region_id].minx\n",
    "    llat=bounds.iloc[region_id].miny\n",
    "    ulon=bounds.iloc[region_id].maxx\n",
    "    ulat=bounds.iloc[region_id].maxy\n",
    "    a_fc=kn_fct.sel(lon=slice(llon, ulon), lat=slice(llat,ulat))\n",
    "    a_obs=kn_obs.sel(lon=slice(llon, ulon), lat=slice(llat,ulat))\n",
    "    hindcast = HindcastEnsemble(a_fc)\n",
    "    hindcast = hindcast.add_observations(a_obs)\n",
    "    #hindcast\n",
    "    #spi_cdb1spi3_prod_name_creator(ds_ens)\n",
    "    a_fc1=hindcast.get_initialized()\n",
    "    a_fc2=a_fc1.isel(lead=lead_int)\n",
    "    if len(season_str) == 3:\n",
    "        spi_prod_list=spi3_prod_name_creator(a_fc2,'valid_time')\n",
    "        obs_spi_prod_list=spi3_prod_name_creator(a_obs,'time')\n",
    "    else:\n",
    "        spi_prod_list=spi4_prod_name_creator(a_fc2,'valid_time')\n",
    "        obs_spi_prod_list=spi4_prod_name_creator(a_obs,'time')\n",
    "    a_fc2 = a_fc2.assign_coords(spi_prod=('init',spi_prod_list))\n",
    "    a_fc3=a_fc2.where(a_fc2.spi_prod==season_str, drop=True)\n",
    "    #obsertations\n",
    "    a_obs1 = a_obs.assign_coords(spi_prod=('time',obs_spi_prod_list))\n",
    "    a_obs2=a_obs1.where(a_obs1.spi_prod==season_str, drop=True)\n",
    "    #valid_time_series = a_fc3.valid_time.to_series().reset_index(drop=True).drop_duplicates()\n",
    "    valid_time_flattened = a_fc2.valid_time.to_dataframe().reset_index().drop_duplicates(subset='valid_time')['valid_time']\n",
    "    valid_time_flattened.columns=['valid_time','cc']\n",
    "    #valid_time_flattened['valid_time'] = pd.to_datetime(valid_time_flattened['valid_time'])\n",
    "    # Apply lambda function to create 'dt1' column\n",
    "    #valid_time_flattened['dt1'] = valid_time_flattened['valid_time'].apply(\n",
    "    #    lambda x: datetime(x.year, x.month, x.day, x.hour, x.minute, x.second)\n",
    "    #)\n",
    "    #\n",
    "    valid_time_flattened['dt1'] =valid_time_flattened['valid_time'].apply(lambda x: datetime(x.year, x.month, x.day,x.hour, x.minute, x.second))\n",
    "    # Ensure the valid_time is in 'YYYY-MM-DD' string format\n",
    "    #valid_time_flattened['dt2'] = valid_time_flattened['dt1'].dt.strftime('%Y-%m-%d')\n",
    "    valid_time_flattened['dt1'] = valid_time_flattened['dt1'].dt.strftime('%Y-%m-%dT%H:%M:%S.%f')\n",
    "    valid_time_flattened['dt1'] = pd.to_datetime(valid_time_flattened['dt1'])\n",
    "    # Convert to xarray DataArray with time as the dimension name\n",
    "    #valid_time_da = xr.DataArray(valid_time_flattened['dt1'], dims=['time'])\n",
    "    valid_time_da = xr.DataArray(valid_time_flattened['dt1'], dims=['time'],coords=valid_time_flattened['dt1'])\n",
    "    a_obs3 = a_obs2.reindex(time=valid_time_da)\n",
    "    #a_obs4 = a_obs3.reindex(time=a_obs2.time)\n",
    "    #a_obs4 = a_obs3.sel(time=a_obs2.time, drop=True)\n",
    "    a_obs3 = a_obs3.dropna(dim='time')\n",
    "    if len(season_str) == 3:\n",
    "        obs_data=a_obs3['spi3']\n",
    "        ens_data=a_fc3['spi3']\n",
    "    else:\n",
    "        obs_data=a_obs3['spi4']\n",
    "        ens_data=a_fc3['spi4']\n",
    "    return obs_data, ens_data\n",
    "\n",
    "\n",
    "def get_threshold(region_id, season, level):\n",
    "    \"\"\"\n",
    "    Retrieves the drought threshold value for a specified region, season, and drought level.\n",
    "\n",
    "    The function reads predefined threshold values from a CSV-format string. It looks up the threshold for the given\n",
    "    region ID, season, and drought level ('mod' for moderate, 'sev' for severe, or 'ext' for extreme). These thresholds\n",
    "    are specific to certain regions and seasons and indicate the level at which a drought event of a particular severity\n",
    "    is considered to occur.\n",
    "\n",
    "    Parameters:\n",
    "    - region_id (int): The integer identifier for the region of interest.\n",
    "    - season (str): The season for which the threshold is required. Expected values are season codes such as 'mam' (March-April-May),\n",
    "                    'jjas' (June-July-August-September), 'ond' (October-November-December), etc.\n",
    "    - level (str): The drought severity level for which the threshold is requested. Valid options are 'mod' for moderate,\n",
    "                   'sev' for severe, and 'ext' for extreme drought conditions.\n",
    "\n",
    "    Returns:\n",
    "    - float: The threshold value for the specified region, season, and drought level. Returns None if no threshold is found for the given inputs.\n",
    "\n",
    "    Note:\n",
    "    - This function uses a hardcoded CSV string as its data source. In a production environment, it's recommended to\n",
    "      store and retrieve such data from a more robust data management system.\n",
    "    - The function requires the pandas library for data manipulation and the StringIO module from io for string-based data input.\n",
    "\n",
    "    Example usage:\n",
    "    >>> threshold = get_threshold(1, 'mam', 'mod')\n",
    "    >>> print(threshold)\n",
    "    -0.14\n",
    "    \"\"\"\n",
    "    data = \"\"\"region_id,region,season,mod,sev,ext\n",
    "    0,kmj,mam,-0.03,-0.56,-0.99\n",
    "    0,kmj,jjas,-0.01,-0.41,-0.99\n",
    "    1,mbt,mam,-0.14,-0.38,-0.8\n",
    "    1,mbt,ond,-0.15,-0.53,-0.71\n",
    "    2,wjr,mam,-0.19,-0.45,-0.75\n",
    "    2,wjr,ond,-0.29,-0.76,-0.9\n",
    "    \"\"\"\n",
    "    # Use StringIO to convert the string data to a file-like object\n",
    "    data_io = StringIO(data)\n",
    "    # Read the data into a pandas DataFrame\n",
    "    df = pd.read_csv(data_io)\n",
    "    thresholds_dict = { (row['region_id'], row['season']): {'mod': row['mod'], 'sev': row['sev'], 'ext': row['ext']}\n",
    "                   for _, row in df.iterrows() }\n",
    "    # Retrieve the dictionary for the given region_id and season\n",
    "    season_thresholds = thresholds_dict.get((region_id, season), {})\n",
    "    # Return the threshold for the given level (mod, sev, ext), or None if not found\n",
    "    return season_thresholds\n",
    "\n",
    "\n",
    "def get_triggers_bin_edges():\n",
    "    \"\"\"\n",
    "    Generate bin edges for triggers based on forecast category edges.\n",
    "\n",
    "    Returns:\n",
    "    list of lists: Bin edges arranged with three elements each.\n",
    "    \"\"\"\n",
    "    forecast_category_edges = np.linspace(0, 1, 101)\n",
    "    # Initialize an empty list to hold your list of lists\n",
    "    list_of_lists = []\n",
    "    # Iterate through forecast_category_edges to construct each [n1, n2, n3]\n",
    "    for i, edge in enumerate(forecast_category_edges):\n",
    "        if i == 0:\n",
    "            # For the first element, there is no lower edge within the range, so you might set n1 to 0 or any other logic\n",
    "            n1 = 0  # or edge itself if you want to keep it within valid probability bounds\n",
    "        else:\n",
    "            n1 = forecast_category_edges[i-1]\n",
    "        n2 = edge  # The current edge value\n",
    "        if i == len(forecast_category_edges) - 1:\n",
    "            # For the last element, there is no upper edge within the range, so you might set n3 to 1 or any other logic\n",
    "            n3 = 1  # or edge itself if you want to keep it within valid probability bounds\n",
    "        else:\n",
    "            n3 = forecast_category_edges[i+1]\n",
    "        # Append the [n1, n2, n3] list to your list of lists\n",
    "        list_of_lists.append([n1, n2, n3])\n",
    "    return list_of_lists\n",
    "\n",
    "def get_thresholds_bin_edges(threshold_dict, lowest_bound=-4.0, highest_bound=4.0):\n",
    "    \"\"\"\n",
    "    Generate bin edges based on provided thresholds, ensuring all sublists have three elements:\n",
    "    [lower_edge, threshold, upper_edge], including the extreme bounds.\n",
    "    \n",
    "    Parameters:\n",
    "    - threshold_dict (dict): Dictionary with levels as keys and thresholds as values.\n",
    "    - lowest_bound (float): Lowest boundary for the bins.\n",
    "    - highest_bound (float): Highest boundary for the bins.\n",
    "    \n",
    "    Returns:\n",
    "    - list of lists: Bin edges arranged with three elements each.\n",
    "    \n",
    "    TODO\n",
    "    merge the dict call on level and then return the sepcific bin edges for that level\n",
    "    \"\"\"\n",
    "    # Extract thresholds and sort them in ascending order\n",
    "    sorted_thresholds = sorted(threshold_dict.values())\n",
    "    \n",
    "    # Initialize list of lists with the first bin\n",
    "    list_of_lists = []\n",
    "    \n",
    "    # Handle the first bin separately\n",
    "    if sorted_thresholds:\n",
    "        list_of_lists.append([lowest_bound, sorted_thresholds[0], sorted_thresholds[1] if len(sorted_thresholds) > 1 else highest_bound])\n",
    "    \n",
    "    # Loop through the sorted thresholds to create bins for the middle thresholds\n",
    "    for i in range(1, len(sorted_thresholds) - 1):\n",
    "        list_of_lists.append([sorted_thresholds[i-1], sorted_thresholds[i], sorted_thresholds[i+1]])\n",
    "    \n",
    "    # Handle the last bin separately if there are at least two thresholds\n",
    "    if len(sorted_thresholds) > 1:\n",
    "        list_of_lists.append([sorted_thresholds[-2], sorted_thresholds[-1], highest_bound])\n",
    "    \n",
    "    # Special case: If there is only one threshold, adjust the initial list to include highest_bound\n",
    "    if len(sorted_thresholds) == 1:\n",
    "        list_of_lists[0][-1] = highest_bound  # Replace the last element of the first sublist with highest_bound\n",
    "    \n",
    "    return list_of_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5640d316-da5b-4318-ba55-c7f6e39fdba8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region_id=0\n",
    "season_str='MAM'\n",
    "level='mod'\n",
    "lead_int=1\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "\n",
    "sc_season_str=season_str.lower()\n",
    "threshold_dict=get_threshold(region_id, sc_season_str, level)\n",
    "\n",
    "obs_be=get_thresholds_bin_edges(threshold_dict, lowest_bound=-4.0, highest_bound=4.0)\n",
    "\n",
    "fcst_be=get_triggers_bin_edges()\n",
    "\n",
    "level_be=obs_be[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281645af-3e8d-4930-8e15-2d92e088c43f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cont_db=[]\n",
    "for tr_be in fcst_be:\n",
    "    a_level_be=np.array(level_be)\n",
    "    a_tr_be=np.array(tr_be)\n",
    "    multicategory_contingency = xs.Contingency(obs_data, ens_data, a_level_be, a_tr_be, dim=[\"lat\",\"lon\",\"member\",'init'])\n",
    "    db=pd.DataFrame()\n",
    "    db['time']=multicategory_contingency.heidke_score()['time'].values\n",
    "    db['bias_score']=multicategory_contingency.bias_score().values\n",
    "    db['false_alarm_ratio']=multicategory_contingency.false_alarm_ratio().values\n",
    "    db['hit_rate']=multicategory_contingency.hit_rate().values\n",
    "    db['heidke_score']=multicategory_contingency.heidke_score().values\n",
    "    db['peirce_score']=multicategory_contingency.peirce_score().values\n",
    "    db.insert(0, 'tr_be', tr_be[1])\n",
    "    cont_db.append(db)\n",
    "    print(tr_be)\n",
    "    \n",
    "    \n",
    "db=pd.concat(cont_db)\n",
    "\n",
    "db.to_csv(f'{data_path}{region_id}_{season_str}_{level}_lt{lead_int}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95819722-95f5-489e-becb-88189e53fa7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## AUROC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154ae4db-7d69-4e1d-b558-c3b17071e636",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "date_strings = ens_data['init'].dt.strftime('%Y-%m-%d %H:%M:%S').values\n",
    "\n",
    "# Now, use pandas to parse these strings into datetime64\n",
    "ens_data['init'] = pd.to_datetime(date_strings)\n",
    "\n",
    "ens_data = ens_data.swap_dims({'init': 'valid_time'})\n",
    "\n",
    "vdate_strings = ens_data['valid_time'].dt.strftime('%Y-%m-%d %H:%M:%S').values\n",
    "\n",
    "# Now, use pandas to parse these strings into datetime64\n",
    "ens_data['valid_time'] = pd.to_datetime(vdate_strings)\n",
    "\n",
    "ens_data = ens_data.rename({'valid_time': 'time'})\n",
    "#ens_data['init'].values\n",
    "#ens_data['valid_time'].values\n",
    "\n",
    "obs_event1 = obs_data >= threshold_dict['mod']\n",
    "#obs_event1['name'] = 'observed_event' \n",
    "obs_event1.name = 'spi3' \n",
    "\n",
    "dfp= (ens_data <= threshold_dict['mod']).mean(dim='member')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7b276c-56d1-4da0-a33b-0022d8062b5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Depending on your workstation specifications, you may need to adjust these values.\n",
    "# On a single machine, n_workers=1 is usually better.\n",
    "client = Client(n_workers=3, threads_per_worker=4, memory_limit=\"2GB\")\n",
    "client\n",
    "#client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b77caf3-bd8c-49ea-8478-36be406f5d59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bs_obs_data,bs_ens_data= block_bootstrap(\n",
    "    obs_event1.chunk(),\n",
    "    dfp.chunk(),\n",
    "    blocks={\"time\":1},\n",
    "    n_iteration=1000,\n",
    "    circular=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c79d07a-b391-4229-ab07-f9916f0ae385",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fpr, tpr, aroc = xs.roc(bs_obs_data,bs_ens_data, bin_edges='continuous',dim=[\"iteration\"],return_results='all_as_metric_dim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdba3787-24ff-4eb7-af5f-e668e30d83dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "maroc = aroc.mean(dim=[\"lat\",\"lon\"])\n",
    "mdb=maroc.to_dataframe()\n",
    "\n",
    "# Calculate the 2.5th percentile across the 'iteration' dimension\n",
    "p2aroc = aroc.quantile(0.025, dim=[\"lat\",\"lon\"])\n",
    "p2db=p2aroc.to_dataframe()\n",
    "\n",
    "# Calculate the 97.5th percentile across the 'iteration' dimension\n",
    "p9aroc = aroc.quantile(0.975, dim=[\"lat\",\"lon\"])\n",
    "p9db=p9aroc.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d192b7b7-1256-4474-a33a-3e5e5d49dea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adb=pd.concat([mdb,p2db,p9db],axis=1)\n",
    "adb.to_csv(f'{data_path}auroc_{region_id}_{season_str}_{level}_lt{lead_int}_a.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5854863-68e1-43cb-93d9-2732b0dab461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c08016-4a61-4f23-b42c-a0605afae33b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
