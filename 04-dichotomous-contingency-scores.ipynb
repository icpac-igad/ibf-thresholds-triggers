{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650c1417-f153-414c-b403-fae1fc395aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import climpred\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regionmask\n",
    "import geopandas as gp\n",
    "from climpred import HindcastEnsemble\n",
    "from datetime import datetime\n",
    "\n",
    "import xhistogram.xarray as xhist\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c173b73-a53c-47a9-bcfc-51e0c09be6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "data_path=os.getenv(\"data_path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300aea11-7745-48a3-a2b6-4790f10965c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The Forecast Evaluation Process\n",
    "\n",
    "Following process is invovled in assessing the accuracy of drought forecasts relative to observed data. \n",
    "\n",
    "1. **Observation and Forecast Data**: The process begins with two primary datasets - observed data (`obs_data`) and ensemble forecast data (`ens_data`).\n",
    "\n",
    "2. **Threshold Determination**: A predefined threshold is used to classify whether a drought event occurred (based on observed and forecasted data). This is the threshold value for different season as discussed in the paper. The used threshold values are indicated in the function `get_threshold`.\n",
    "\n",
    "3. **Dichotomous Events Creation**:\n",
    "   - **Observed Events**: An event is flagged in the observed data (`obs_event1`) if the observed value is less than or equal to the threshold, indicating the occurrence of a drought. The less than threshold value is following Gabriela et.al 2023. \n",
    "   - **Forecasted Events**: Instead of averaging ensemble forecasts, an empirical probability is calculated. This reflects the proportion of ensemble members predicting values at or below the given threshold.\n",
    "\n",
    "4. **Probability Threshold for Forecasts**: The forecast data is then classified into dichotomous events based on a trigger value. This step converts the forecast probability into a binary outcome - whether a drought is forecasted to occur or not. In the process, all the possible trigger value between 0 to 1 is subject to testing as in the line `trigger_values = xr.DataArray(np.linspace(0, 1, num=100), dims=['trigger_value'])`. \n",
    "\n",
    "5. **Contingency Table Construction**: Using `xhist.histogram`, a 2D histogram (contingency table) is created from the binary observed and forecasted events. This table quantifies the relationship between observed occurrences/non-occurrences and forecasted occurrences/non-occurrences of drought.\n",
    "\n",
    "6. **Skill Score Calculation**: From the contingency table, various skill scores are calculated, including hit rates, false alarm ratios, and others. These metrics provide a comprehensive view of forecast accuracy and reliability.\n",
    "\n",
    "7. **AUROC Calculation**: The Area Under the Receiver Operating Characteristic (AUROC) score is calculated, offering a measure of the forecast's ability to discriminate between the occurrence and non-occurrence of drought events. Bootstrap method was used and it is the expensive step, which one of the reason to hinders to do year wise score calcualtion. \n",
    "\n",
    "8. **Final Output**: The process culminates in the generation of a DataFrame summarizing the calculated skill scores for various trigger values, which is then saved to a CSV file. This summary facilitates the evaluation of forecast performance across different probability thresholds.\n",
    "\n",
    "9. **Lack of Yearly Variability Consideration**: The method evaluates forecasts against observations over the entire 43-year period without segregating by individual years. This could mask interannual variability or trends in forecast skill. However the bootstrap compute cost along with exponantial increase in comparision hinders to do, need further discussion. \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## The summary of steps are as follows\n",
    "\n",
    "1. **Load Datasets**: Based on the provided `season_str` length, either SPI3 or SPI4 datasets are loaded for both forecast (`kn_fct`) and observed (`kn_obs`) data from a specified `data_path`.\n",
    "\n",
    "2. **Generate Region Masks**: Utilizes `ken_mask_creator()` to create masks for specified regions or districts using the `regionmask` library, facilitating region-specific analyses.\n",
    "\n",
    "3. **Select Region**: Extracts geographical bounds from a combined GeoDataFrame and selects forecast and observation data within these bounds for the specified `region_id`.\n",
    "\n",
    "4. **Initialize Hindcast Ensemble**: Creates a `HindcastEnsemble` object with the selected forecast data and adds the corresponding observations to it.\n",
    "\n",
    "5. **Subset for Lead Time**: Subsets the forecast data for the given `lead_int`, ensuring analysis is conducted at the specified forecast lead time.\n",
    "\n",
    "6. **Generate Seasonal Product Names**: Depending on `season_str` length, either `spi3_prod_name_creator` or `spi4_prod_`name_creator` is called to generate a list of seasonal product names for both forecast and observed data, facilitating season-specific filtering.\n",
    "\n",
    "7. **Assign and Filter by Season**: Assigns generated seasonal product names as coordinates to both datasets and filters them to include only data corresponding to the specified `season_str`.\n",
    "\n",
    "8. **Align Time Coordinates**: Aligns the observed dataset time coordinates with the forecast dataset's valid time coordinates, ensuring that both datasets are comparable in time for analysis.\n",
    "\n",
    "9. **Drop NaNs and Subset Data**: Drops any NaN values that may have resulted from the reindexing process and subsets the data to include only the relevant SPI variable (`spi3` or `spi4`), based on `season_str` length.\n",
    "\n",
    "10. **Calculate Scores**: The `score_gen` function then takes these prepared datasets along with the `region_id`, `season_str`, and `level` to calculate various verification scores and saves the results to a CSV file.\n",
    "\n",
    "This process ensures that the observed and forecasted datasets are correctly prepared and aligned for a specified region, season, and lead time, allowing for the accurate calculation of verification scores such as hit rates, false alarm ratios, and AUROC scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97eb6c3-359f-4e75-9383-d7d20dac6a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def ken_mask_creator():\n",
    "    \"\"\"\n",
    "    Utiliity for generating region/district masks using regionmask library\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    the_mask : TYPE\n",
    "        DESCRIPTION.\n",
    "    rl_dict : TYPE\n",
    "        DESCRIPTION.\n",
    "\n",
    "    \"\"\"\n",
    "    dis=gp.read_file(f'{data_path}Karamoja_boundary_dissolved.shp')\n",
    "    mbt_path=os.getenv(\"mbt_path\")\n",
    "    reg=gp.read_file(f'{data_path}wajir_mbt_extent.shp')\n",
    "    mds=pd.concat([dis,reg])\n",
    "    mds1=mds.reset_index()\n",
    "    mds1['region']=[0,1,2]\n",
    "    mds1['region_name']=['Karamoja', 'Marsabit','Wajir']\n",
    "    mds2=mds1[['geometry','region','region_name']]\n",
    "    rl_dict=dict(zip(mds2.region, mds2.region_name))\n",
    "    the_mask = regionmask.from_geopandas(mds2,numbers='region',overlap=True)\n",
    "    return the_mask, rl_dict, mds2\n",
    "\n",
    "def spi3_prod_name_creator(ds_ens,var_name):\n",
    "    \"\"\"\n",
    "    Convenience function to generate a list of SPI product\n",
    "    names, such as MAM, so that can be used to filter the \n",
    "    SPI product from dataframe\n",
    "\n",
    "    added with method to convert the valid_time in CF format into datetime at\n",
    "    line 3, which is the format given by climpred valid_time calculation \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds_ens : xarray dataframe\n",
    "        The data farme with SPI output organized for \n",
    "        the period 1981-2023.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    spi_prod_list : String list\n",
    "        List of names with iteration of SPI3 product names such as\n",
    "        ['JFM','FMA','MAM',......]\n",
    "\n",
    "    \"\"\"\n",
    "    db=pd.DataFrame()\n",
    "    db['dt']=ds_ens[var_name].values\n",
    "    db['dt1'] = db['dt'].apply(lambda x: datetime(x.year, x.month, x.day,\n",
    "                                                                     x.hour, x.minute, x.second))\n",
    "    #db['dt1']=db['dt'].to_datetimeindex()\n",
    "    db['month']=db['dt1'].dt.strftime('%b').astype(str).str[0]\n",
    "    db['year']=db['dt1'].dt.strftime('%Y')\n",
    "    db['spi_prod'] = db.groupby('year')['month'].shift(2)+db.groupby('year')['month'].shift(1) + db.groupby('year')['month'].shift(0)\n",
    "    spi_prod_list=db['spi_prod'].tolist()\n",
    "    return spi_prod_list\n",
    "\n",
    "\n",
    "def spi4_prod_name_creator(ds_ens,var_name):\n",
    "    \"\"\"\n",
    "    Convenience function to generate a list of SPI product\n",
    "    names, such as MAM, so that can be used to filter the \n",
    "    SPI product from dataframe\n",
    "\n",
    "    added with method to convert the valid_time in CF format into datetime at\n",
    "    line 3, which is the format given by climpred valid_time calculation \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds_ens : xarray dataframe\n",
    "        The data farme with SPI output organized for \n",
    "        the period 1981-2023.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    spi_prod_list : String list\n",
    "        List of names with iteration of SPI3 product names such as\n",
    "        ['JFM','FMA','MAM',......]\n",
    "\n",
    "    \"\"\"\n",
    "    db=pd.DataFrame()\n",
    "    db['dt']=ds_ens[var_name].values\n",
    "    db['dt1'] = db['dt'].apply(lambda x: datetime(x.year, x.month, x.day,\n",
    "                                                                     x.hour, x.minute, x.second))\n",
    "    #db['dt1']=db['dt'].to_datetimeindex()\n",
    "    db['month']=db['dt1'].dt.strftime('%b').astype(str).str[0]\n",
    "    db['year']=db['dt1'].dt.strftime('%Y')\n",
    "    db['spi_prod'] = db.groupby('year')['month'].shift(3)+db.groupby('year')['month'].shift(2)+db.groupby('year')['month'].shift(1) + db.groupby('year')['month'].shift(0)\n",
    "    spi_prod_list=db['spi_prod'].tolist()\n",
    "    return spi_prod_list\n",
    "\n",
    "\n",
    "def make_obs_fct_dataset(region_id,season_str,lead_int):\n",
    "    \"\"\"\n",
    "    Prepares observed and forecasted dataset subsets for a specific region, season, and lead time.\n",
    "\n",
    "    This function loads observed and forecasted datasets based on the season string length (indicating SPI3 or SPI4),\n",
    "    applies regional masking, selects the data for the given region by its ID, and subsets the data for the specified\n",
    "    season and lead time. It then aligns the observed dataset time coordinates with the forecasted dataset valid time\n",
    "    coordinates and returns both datasets.\n",
    "\n",
    "    Parameters:\n",
    "    - region_id (int): The identifier for the region of interest.\n",
    "    - season_str (str): A string representing the season. The length of this string determines whether SPI3 or SPI4\n",
    "                        datasets are used ('mam', 'jjas', etc. for SPI3, and longer strings for SPI4).\n",
    "    - lead_int (int): The lead time index for which the forecast dataset is to be subset.\n",
    "\n",
    "    Returns:\n",
    "    - obs_data (xarray.DataArray): The subsetted observed data array for the specified region, season, and aligned time coordinates.\n",
    "    - ens_data (xarray.DataArray): The subsetted forecast data array for the specified region, season, lead time, and aligned time coordinates.\n",
    "\n",
    "    Notes:\n",
    "    - The function assumes the existence of a `data_path` variable that specifies the base path to the dataset files.\n",
    "    - It requires the `xarray` library for data manipulation and assumes specific naming conventions for the dataset files.\n",
    "    - Regional masking and season-specific processing rely on externally defined functions and naming conventions.\n",
    "    - The final alignment of observed dataset time coordinates with forecasted dataset valid time coordinates ensures\n",
    "      comparability between observed and forecasted values for verification purposes.\n",
    "\n",
    "    Example Usage:\n",
    "    >>> obs_data, ens_data = make_obs_fct_dataset(1, 'mam', 0)\n",
    "    >>> print(obs_data)\n",
    "    >>> print(ens_data)\n",
    "\n",
    "    This would load the observed and forecasted SPI3 datasets for region 1 during the 'mam' season and subset them\n",
    "    for lead time index 0, aligning the observed data time coordinates with the forecasted data valid time coordinates.\n",
    "    \"\"\"\n",
    "    if len(season_str) == 3:\n",
    "        kn_fct=xr.open_dataset(f'{data_path}kn_fct_spi3.nc')\n",
    "        kn_obs=xr.open_dataset(f'{data_path}kn_obs_spi3.nc')\n",
    "    else:\n",
    "        kn_fct=xr.open_dataset(f'{data_path}kn_fct_spi4.nc')\n",
    "        kn_obs=xr.open_dataset(f'{data_path}kn_obs_spi4.nc')\n",
    "    the_mask, rl_dict,mds1=ken_mask_creator()\n",
    "    bounds = mds1.bounds\n",
    "    #bounds.iloc[0].minx\n",
    "    llon=bounds.iloc[region_id].minx\n",
    "    llat=bounds.iloc[region_id].miny\n",
    "    ulon=bounds.iloc[region_id].maxx\n",
    "    ulat=bounds.iloc[region_id].maxy\n",
    "    a_fc=kn_fct.sel(lon=slice(llon, ulon), lat=slice(llat,ulat))\n",
    "    a_obs=kn_obs.sel(lon=slice(llon, ulon), lat=slice(llat,ulat))\n",
    "    hindcast = HindcastEnsemble(a_fc)\n",
    "    hindcast = hindcast.add_observations(a_obs)\n",
    "    #hindcast\n",
    "    #spi_cdb1spi3_prod_name_creator(ds_ens)\n",
    "    a_fc1=hindcast.get_initialized()\n",
    "    a_fc2=a_fc1.isel(lead=lead_int)\n",
    "    if len(season_str) == 3:\n",
    "        spi_prod_list=spi3_prod_name_creator(a_fc2,'valid_time')\n",
    "        obs_spi_prod_list=spi3_prod_name_creator(a_obs,'time')\n",
    "    else:\n",
    "        spi_prod_list=spi4_prod_name_creator(a_fc2,'valid_time')\n",
    "        obs_spi_prod_list=spi4_prod_name_creator(a_obs,'time')\n",
    "    a_fc2 = a_fc2.assign_coords(spi_prod=('init',spi_prod_list))\n",
    "    a_fc3=a_fc2.where(a_fc2.spi_prod==season_str, drop=True)\n",
    "    #obsertations\n",
    "    a_obs1 = a_obs.assign_coords(spi_prod=('time',obs_spi_prod_list))\n",
    "    a_obs2=a_obs1.where(a_obs1.spi_prod==season_str, drop=True)\n",
    "    #valid_time_series = a_fc3.valid_time.to_series().reset_index(drop=True).drop_duplicates()\n",
    "    valid_time_flattened = a_fc2.valid_time.to_dataframe().reset_index().drop_duplicates(subset='valid_time')['valid_time']\n",
    "    valid_time_flattened.columns=['valid_time','cc']\n",
    "    #valid_time_flattened['valid_time'] = pd.to_datetime(valid_time_flattened['valid_time'])\n",
    "    # Apply lambda function to create 'dt1' column\n",
    "    #valid_time_flattened['dt1'] = valid_time_flattened['valid_time'].apply(\n",
    "    #    lambda x: datetime(x.year, x.month, x.day, x.hour, x.minute, x.second)\n",
    "    #)\n",
    "    #\n",
    "    valid_time_flattened['dt1'] =valid_time_flattened['valid_time'].apply(lambda x: datetime(x.year, x.month, x.day,x.hour, x.minute, x.second))\n",
    "    # Ensure the valid_time is in 'YYYY-MM-DD' string format\n",
    "    #valid_time_flattened['dt2'] = valid_time_flattened['dt1'].dt.strftime('%Y-%m-%d')\n",
    "    valid_time_flattened['dt1'] = valid_time_flattened['dt1'].dt.strftime('%Y-%m-%dT%H:%M:%S.%f')\n",
    "    valid_time_flattened['dt1'] = pd.to_datetime(valid_time_flattened['dt1'])\n",
    "    # Convert to xarray DataArray with time as the dimension name\n",
    "    #valid_time_da = xr.DataArray(valid_time_flattened['dt1'], dims=['time'])\n",
    "    valid_time_da = xr.DataArray(valid_time_flattened['dt1'], dims=['time'],coords=valid_time_flattened['dt1'])\n",
    "    a_obs3 = a_obs2.reindex(time=valid_time_da)\n",
    "    #a_obs4 = a_obs3.reindex(time=a_obs2.time)\n",
    "    #a_obs4 = a_obs3.sel(time=a_obs2.time, drop=True)\n",
    "    a_obs3 = a_obs3.dropna(dim='time')\n",
    "    if len(season_str) == 3:\n",
    "        obs_data=a_obs3['spi3']\n",
    "        ens_data=a_fc3['spi3']\n",
    "    else:\n",
    "        obs_data=a_obs3['spi4']\n",
    "        ens_data=a_fc3['spi4']\n",
    "    return obs_data, ens_data\n",
    "\n",
    "\n",
    "# Calculate AUROC using bootstrap\n",
    "def calculate_auroc(hits, misses, false_alarms, correct_negatives):\n",
    "    \"\"\"\n",
    "    Calculates the Area Under the Receiver Operating Characteristic (AUROC) curve for a set of forecasts relative to observations.\n",
    "\n",
    "    This function computes the AUROC score as a measure of the forecast's ability to discriminate between two classes:\n",
    "    events that occurred (drought) and events that did not occur (no drought). The AUROC score ranges from 0 to 1,\n",
    "    where a score of 0.5 suggests no discriminative ability (equivalent to random chance), and a score of 1 indicates perfect discrimination.\n",
    "\n",
    "    Parameters:\n",
    "    - hits (int): The number of correctly forecasted events (true positives).\n",
    "    - misses (int): The number of events that were observed but not forecasted (false negatives).\n",
    "    - false_alarms (int): The number of non-events that were incorrectly forecasted as events (false positives).\n",
    "    - correct_negatives (int): The number of non-events that were correctly forecasted (true negatives).\n",
    "\n",
    "    Returns:\n",
    "    - auroc (float): The calculated AUROC score for the given contingency table values.\n",
    "\n",
    "    Note:\n",
    "    - This function is designed to work with binary classification problems, such as predicting the occurrence or non-occurrence of drought events.\n",
    "    - It requires the `roc_auc_score` function from the `sklearn.metrics` module and `numpy` for handling arrays.\n",
    "\n",
    "    Example usage:\n",
    "    >>> auroc_score = calculate_auroc(50, 30, 20, 100)\n",
    "    >>> print(f\"AUROC Score: {auroc_score}\")\n",
    "    \"\"\"\n",
    "    total_positives = hits + misses\n",
    "    total_negatives = correct_negatives + false_alarms\n",
    "    y_true = np.concatenate((np.ones(total_positives), np.zeros(total_negatives)))\n",
    "    y_scores = np.concatenate((np.ones(hits), np.zeros(misses + false_alarms + correct_negatives)))\n",
    "    auroc = roc_auc_score(y_true, y_scores)\n",
    "    return auroc\n",
    "\n",
    "# Define the event threshold\n",
    "#threshold = -0.14\n",
    "\n",
    "\n",
    "def get_threshold(region_id, season, level):\n",
    "    \"\"\"\n",
    "    Retrieves the drought threshold value for a specified region, season, and drought level.\n",
    "\n",
    "    The function reads predefined threshold values from a CSV-format string. It looks up the threshold for the given\n",
    "    region ID, season, and drought level ('mod' for moderate, 'sev' for severe, or 'ext' for extreme). These thresholds\n",
    "    are specific to certain regions and seasons and indicate the level at which a drought event of a particular severity\n",
    "    is considered to occur.\n",
    "\n",
    "    Parameters:\n",
    "    - region_id (int): The integer identifier for the region of interest.\n",
    "    - season (str): The season for which the threshold is required. Expected values are season codes such as 'mam' (March-April-May),\n",
    "                    'jjas' (June-July-August-September), 'ond' (October-November-December), etc.\n",
    "    - level (str): The drought severity level for which the threshold is requested. Valid options are 'mod' for moderate,\n",
    "                   'sev' for severe, and 'ext' for extreme drought conditions.\n",
    "\n",
    "    Returns:\n",
    "    - float: The threshold value for the specified region, season, and drought level. Returns None if no threshold is found for the given inputs.\n",
    "\n",
    "    Note:\n",
    "    - This function uses a hardcoded CSV string as its data source. In a production environment, it's recommended to\n",
    "      store and retrieve such data from a more robust data management system.\n",
    "    - The function requires the pandas library for data manipulation and the StringIO module from io for string-based data input.\n",
    "\n",
    "    Example usage:\n",
    "    >>> threshold = get_threshold(1, 'mam', 'mod')\n",
    "    >>> print(threshold)\n",
    "    -0.14\n",
    "    \"\"\"\n",
    "    data = \"\"\"region_id,region,season,mod,sev,ext\n",
    "    0,kmj,mam,-0.03,-0.56,-0.99\n",
    "    0,kmj,jjas,-0.01,-0.41,-0.99\n",
    "    1,mbt,mam,-0.14,-0.38,-0.8\n",
    "    1,mbt,ond,-0.15,-0.53,-0.71\n",
    "    2,wjr,mam,-0.19,-0.45,-0.75\n",
    "    2,wjr,ond,-0.29,-0.76,-0.9\n",
    "    \"\"\"\n",
    "    # Use StringIO to convert the string data to a file-like object\n",
    "    data_io = StringIO(data)\n",
    "    # Read the data into a pandas DataFrame\n",
    "    df = pd.read_csv(data_io)\n",
    "    thresholds_dict = { (row['region_id'], row['season']): {'mod': row['mod'], 'sev': row['sev'], 'ext': row['ext']}\n",
    "                   for _, row in df.iterrows() }\n",
    "    # Retrieve the dictionary for the given region_id and season\n",
    "    season_thresholds = thresholds_dict.get((region_id, season), {})\n",
    "    # Return the threshold for the given level (mod, sev, ext), or None if not found\n",
    "    return season_thresholds.get(level)\n",
    "\n",
    "\n",
    "def score_gen(obs_data,ens_data,lead_int,region_id,season_str,level):\n",
    "    \"\"\"\n",
    "    Generates scores for evaluating the performance of drought forecasts based on observed data and ensemble forecast data.\n",
    "\n",
    "    Parameters:\n",
    "    - obs_data (xarray.DataArray): The observed data array.\n",
    "    - ens_data (xarray.DataArray): The ensemble forecast data array.\n",
    "    - lead_int (int): Lead time integer indicating the forecast lead time.\n",
    "    - region_id (str): A string identifier for the region of interest.\n",
    "    - season_str (str): A string representing the season (e.g., 'JJA' for June-July-August).\n",
    "    - level (int/float): The threshold level to define a drought event.\n",
    "\n",
    "    The function processes the observed and forecast data to compute various skill scores and area under the ROC curve (AUROC) \n",
    "    scores for different trigger values ranging from 0 to 1. These scores include hit rates, false alarm ratios, bias scores, Hanssen and \n",
    "    Kuipers scores, Heidke skill scores, and AUROC scores along with their confidence intervals.\n",
    "\n",
    "    The results are saved to a CSV file named with the pattern 'regionid_season_level_ltleadtime.csv' in a specified data path, and also returned as a pandas DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): A DataFrame containing the computed scores for each trigger value along with the threshold, hit rates, false alarm ratios,\n",
    "    bias scores, Hanssen and Kuipers scores, Heidke skill scores, AUROC scores, and AUROC confidence intervals.\n",
    "    \n",
    "    Note:\n",
    "    - This function requires the xarray, numpy, pandas, and xhistogram libraries.\n",
    "    - Ensure that `data_path` variable is defined in your environment to specify the output directory for the CSV file.\n",
    "    \"\"\"\n",
    "    sc_season_str=season_str.lower()\n",
    "    threshold=get_threshold(region_id, sc_season_str, level)\n",
    "    # Define the trigger values\n",
    "    trigger_values = xr.DataArray(np.linspace(0, 1, num=100), dims=['trigger_value'])\n",
    "    # Initialize arrays to store scores\n",
    "    #threshold_val=\n",
    "    hit_rates = np.zeros_like(trigger_values)\n",
    "    false_alarm_ratios = np.zeros_like(trigger_values)\n",
    "    bias_scores = np.zeros_like(trigger_values)\n",
    "    hanssen_kuipers_scores = np.zeros_like(trigger_values)\n",
    "    heidke_skill_scores = np.zeros_like(trigger_values)\n",
    "    #auroc_scores = np.zeros_like(trigger_values)\n",
    "    #auroc_lb = np.zeros_like(trigger_values)\n",
    "    #auroc_ub = np.zeros_like(trigger_values)\n",
    "    for i, trigger_value in enumerate(trigger_values):\n",
    "        # Calculate the dichotomous event for observation and forecast\n",
    "        obs_event1 = obs_data <= threshold\n",
    "        #obs_event1['name'] = 'observed_event' \n",
    "        obs_event1.name = 'observed_event' \n",
    "\n",
    "        drought_forecast_probablity= (ens_data <= threshold).mean(dim='member')\n",
    "\n",
    "        #forecast_event1 = ens_data.mean(dim='member') >= trigger_value\n",
    "        forecast_event1=drought_forecast_probablity>=trigger_value\n",
    "        \n",
    "        #forecast_event1['name'] = 'forecasted_event'\n",
    "        forecast_event1.name = 'forecasted_event'\n",
    "\n",
    "        # Create a 2D histogram using xhistogram\n",
    "        contingency_table = xhist.histogram(\n",
    "            #obs_event.stack(point=['time', 'lat', 'lon']),\n",
    "            #forecast_event.stack(point=['time', 'lat', 'lon']),\n",
    "            obs_event1,\n",
    "            forecast_event1,\n",
    "            bins=[2, 2],\n",
    "            density=False\n",
    "        )\n",
    "    \n",
    "        # Extract contingency table counts\n",
    "        contingency_table = contingency_table.data\n",
    "        correct_negatives = contingency_table[0, 0]\n",
    "        false_alarms = contingency_table[0, 1]\n",
    "        misses = contingency_table[1, 0]\n",
    "        hits = contingency_table[1, 1]\n",
    "    \n",
    "        # Calculate scores\n",
    "        total = hits + false_alarms + misses + correct_negatives\n",
    "        hit_rates[i] = hits / (hits + misses) if (hits + misses) > 0 else np.nan\n",
    "        false_alarm_ratios[i] = false_alarms / (false_alarms + hits) if (false_alarms + hits) > 0 else np.nan\n",
    "        #false_alarm_ratios[i] = false_alarms / (false_alarms + correct_negatives) if (false_alarms + correct_negatives) > 0 else np.nan\n",
    "        bias_scores[i] = (hits + false_alarms) / (hits + misses) if (hits + misses) > 0 else np.nan\n",
    "        hanssen_kuipers_scores[i] = hit_rates[i] - false_alarm_ratios[i]\n",
    "        heidke_skill_scores[i] = (hits * correct_negatives - misses * false_alarms) / total\n",
    "        # calculate auroc\n",
    "        auroc_bootstrap_scores = []\n",
    "        n_bootstrap = 1000\n",
    "        for _ in range(n_bootstrap):\n",
    "            bootstrap_counts = np.random.multinomial(\n",
    "                hits + misses + false_alarms + correct_negatives,\n",
    "                [hits, misses, false_alarms, correct_negatives] / (hits + misses + false_alarms + correct_negatives),\n",
    "                size=1\n",
    "            )\n",
    "            bootstrap_hits, bootstrap_misses, bootstrap_false_alarms, bootstrap_correct_negatives = bootstrap_counts[0]\n",
    "            auroc_bootstrap_scores.append(calculate_auroc(bootstrap_hits, bootstrap_misses, bootstrap_false_alarms, bootstrap_correct_negatives))\n",
    "    \n",
    "        auroc_scores[i] = np.mean(auroc_bootstrap_scores)\n",
    "        auroc_lb[i], auroc_ub[i] = np.percentile(auroc_bootstrap_scores, [2.5, 97.5])\n",
    "        print(i)\n",
    "    df = pd.DataFrame({\n",
    "        'hit_rates': hit_rates,\n",
    "        'false_alarm_ratios': false_alarm_ratios,\n",
    "        'bias_scores': bias_scores,\n",
    "        'hanssen_kuipers_scores': hanssen_kuipers_scores,\n",
    "        'heidke_skill_scores': heidke_skill_scores,\n",
    "        'auroc_scores': auroc_scores,\n",
    "        'auroc_lb': auroc_lb,\n",
    "        'auroc_ub': auroc_ub\n",
    "    })\n",
    "    df['trigger_values']=trigger_values\n",
    "    df.insert(0, 'threshold', threshold)\n",
    "    df.insert(0, 'trigger_values', trigger_values)\n",
    "    df.to_csv(f'{data_path}{region_id}_{season_str}_{level}_lt{lead_int}.csv')\n",
    "    return df\n",
    "\n",
    "\n",
    "region_id=0\n",
    "season_str='MAM'\n",
    "level='mod'\n",
    "lead_int=1\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "df=score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6d562b-ef68-4cc9-9da7-e5c030d2c0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## do for remaining 89 season/lead time/region combination "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081ebbb9-4500-4ae1-91d0-86acc07bd67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "##############\n",
    "###region 0\n",
    "#mod##################\n",
    "region_id=0\n",
    "season_str='MAM'\n",
    "level='mod'\n",
    "lead_int=1\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "\n",
    "region_id=0\n",
    "season_str='MAM'\n",
    "level='mod'\n",
    "lead_int=2\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=0\n",
    "season_str='MAM'\n",
    "level='mod'\n",
    "lead_int=3\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=0\n",
    "season_str='MAM'\n",
    "level='mod'\n",
    "lead_int=4\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=0\n",
    "season_str='MAM'\n",
    "level='mod'\n",
    "lead_int=5\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "#sev##################\n",
    "\n",
    "region_id=0\n",
    "season_str='MAM'\n",
    "level='sev'\n",
    "lead_int=1\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "\n",
    "region_id=0\n",
    "season_str='MAM'\n",
    "level='sev'\n",
    "lead_int=2\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=0\n",
    "season_str='MAM'\n",
    "level='sev'\n",
    "lead_int=3\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=0\n",
    "season_str='MAM'\n",
    "level='sev'\n",
    "lead_int=4\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=0\n",
    "season_str='MAM'\n",
    "level='sev'\n",
    "lead_int=5\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "#ext##################\n",
    "region_id=0\n",
    "season_str='MAM'\n",
    "level='ext'\n",
    "lead_int=1\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "\n",
    "region_id=0\n",
    "season_str='MAM'\n",
    "level='ext'\n",
    "lead_int=2\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=0\n",
    "season_str='MAM'\n",
    "level='ext'\n",
    "lead_int=3\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=0\n",
    "season_str='MAM'\n",
    "level='ext'\n",
    "lead_int=4\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=0\n",
    "season_str='MAM'\n",
    "level='ext'\n",
    "lead_int=5\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "##############\n",
    "##############\n",
    "###region 1\n",
    "\n",
    "region_id=1\n",
    "season_str='MAM'\n",
    "level='mod'\n",
    "lead_int=1\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "\n",
    "region_id=1\n",
    "season_str='MAM'\n",
    "level='mod'\n",
    "lead_int=2\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=1\n",
    "season_str='MAM'\n",
    "level='mod'\n",
    "lead_int=3\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=1\n",
    "season_str='MAM'\n",
    "level='mod'\n",
    "lead_int=4\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=1\n",
    "season_str='MAM'\n",
    "level='mod'\n",
    "lead_int=5\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "#sev##################\n",
    "\n",
    "region_id=1\n",
    "season_str='MAM'\n",
    "level='sev'\n",
    "lead_int=1\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "\n",
    "region_id=1\n",
    "season_str='MAM'\n",
    "level='sev'\n",
    "lead_int=2\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=1\n",
    "season_str='MAM'\n",
    "level='sev'\n",
    "lead_int=3\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=1\n",
    "season_str='MAM'\n",
    "level='sev'\n",
    "lead_int=4\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=1\n",
    "season_str='MAM'\n",
    "level='sev'\n",
    "lead_int=5\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "#ext##################\n",
    "region_id=1\n",
    "season_str='MAM'\n",
    "level='ext'\n",
    "lead_int=1\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "\n",
    "region_id=1\n",
    "season_str='MAM'\n",
    "level='ext'\n",
    "lead_int=2\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=1\n",
    "season_str='MAM'\n",
    "level='ext'\n",
    "lead_int=3\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=1\n",
    "season_str='MAM'\n",
    "level='ext'\n",
    "lead_int=4\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=1\n",
    "season_str='MAM'\n",
    "level='ext'\n",
    "lead_int=5\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "##############\n",
    "##############\n",
    "###region 2\n",
    "\n",
    "region_id=2\n",
    "season_str='MAM'\n",
    "level='mod'\n",
    "lead_int=1\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "\n",
    "region_id=2\n",
    "season_str='MAM'\n",
    "level='mod'\n",
    "lead_int=2\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=2\n",
    "season_str='MAM'\n",
    "level='mod'\n",
    "lead_int=3\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=2\n",
    "season_str='MAM'\n",
    "level='mod'\n",
    "lead_int=4\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=2\n",
    "season_str='MAM'\n",
    "level='mod'\n",
    "lead_int=5\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "#sev##################\n",
    "\n",
    "region_id=2\n",
    "season_str='MAM'\n",
    "level='sev'\n",
    "lead_int=1\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "\n",
    "region_id=2\n",
    "season_str='MAM'\n",
    "level='sev'\n",
    "lead_int=2\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=2\n",
    "season_str='MAM'\n",
    "level='sev'\n",
    "lead_int=3\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=2\n",
    "season_str='MAM'\n",
    "level='sev'\n",
    "lead_int=4\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=2\n",
    "season_str='MAM'\n",
    "level='sev'\n",
    "lead_int=5\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "#ext##################\n",
    "region_id=2\n",
    "season_str='MAM'\n",
    "level='ext'\n",
    "lead_int=1\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "\n",
    "region_id=2\n",
    "season_str='MAM'\n",
    "level='ext'\n",
    "lead_int=2\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=2\n",
    "season_str='MAM'\n",
    "level='ext'\n",
    "lead_int=3\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=2\n",
    "season_str='MAM'\n",
    "level='ext'\n",
    "lead_int=4\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=2\n",
    "season_str='MAM'\n",
    "level='ext'\n",
    "lead_int=5\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "#OND\n",
    "##############\n",
    "##############\n",
    "###region 1\n",
    "\n",
    "region_id=1\n",
    "season_str='OND'\n",
    "level='mod'\n",
    "lead_int=1\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "\n",
    "region_id=1\n",
    "season_str='OND'\n",
    "level='mod'\n",
    "lead_int=2\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=1\n",
    "season_str='OND'\n",
    "level='mod'\n",
    "lead_int=3\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=1\n",
    "season_str='OND'\n",
    "level='mod'\n",
    "lead_int=4\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=1\n",
    "season_str='OND'\n",
    "level='mod'\n",
    "lead_int=5\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "#sev##################\n",
    "\n",
    "region_id=1\n",
    "season_str='OND'\n",
    "level='sev'\n",
    "lead_int=1\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "\n",
    "region_id=1\n",
    "season_str='OND'\n",
    "level='sev'\n",
    "lead_int=2\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=1\n",
    "season_str='OND'\n",
    "level='sev'\n",
    "lead_int=3\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=1\n",
    "season_str='OND'\n",
    "level='sev'\n",
    "lead_int=4\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=1\n",
    "season_str='OND'\n",
    "level='sev'\n",
    "lead_int=5\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "#ext##################\n",
    "region_id=1\n",
    "season_str='OND'\n",
    "level='ext'\n",
    "lead_int=1\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "\n",
    "region_id=1\n",
    "season_str='OND'\n",
    "level='ext'\n",
    "lead_int=2\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=1\n",
    "season_str='OND'\n",
    "level='ext'\n",
    "lead_int=3\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=1\n",
    "season_str='OND'\n",
    "level='ext'\n",
    "lead_int=4\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=1\n",
    "season_str='OND'\n",
    "level='ext'\n",
    "lead_int=5\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "##############\n",
    "##############\n",
    "###region 2\n",
    "\n",
    "region_id=2\n",
    "season_str='OND'\n",
    "level='mod'\n",
    "lead_int=1\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "\n",
    "region_id=2\n",
    "season_str='OND'\n",
    "level='mod'\n",
    "lead_int=2\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=2\n",
    "season_str='OND'\n",
    "level='mod'\n",
    "lead_int=3\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=2\n",
    "season_str='OND'\n",
    "level='mod'\n",
    "lead_int=4\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=2\n",
    "season_str='OND'\n",
    "level='mod'\n",
    "lead_int=5\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "#sev##################\n",
    "\n",
    "region_id=2\n",
    "season_str='OND'\n",
    "level='sev'\n",
    "lead_int=1\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "\n",
    "region_id=2\n",
    "season_str='OND'\n",
    "level='sev'\n",
    "lead_int=2\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=2\n",
    "season_str='OND'\n",
    "level='sev'\n",
    "lead_int=3\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=2\n",
    "season_str='OND'\n",
    "level='sev'\n",
    "lead_int=4\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=2\n",
    "season_str='OND'\n",
    "level='sev'\n",
    "lead_int=5\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "#ext##################\n",
    "region_id=2\n",
    "season_str='OND'\n",
    "level='ext'\n",
    "lead_int=1\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "\n",
    "region_id=2\n",
    "season_str='OND'\n",
    "level='ext'\n",
    "lead_int=2\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=2\n",
    "season_str='OND'\n",
    "level='ext'\n",
    "lead_int=3\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=2\n",
    "season_str='OND'\n",
    "level='ext'\n",
    "lead_int=4\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=2\n",
    "season_str='OND'\n",
    "level='ext'\n",
    "lead_int=5\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "###JJAS\n",
    "#######\n",
    "\n",
    "#JJAS\n",
    "##############\n",
    "##############\n",
    "###region 0\n",
    "\n",
    "region_id=0\n",
    "season_str='JJAS'\n",
    "level='mod'\n",
    "lead_int=0\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "\n",
    "region_id=0\n",
    "season_str='JJAS'\n",
    "level='mod'\n",
    "lead_int=2\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=0\n",
    "season_str='JJAS'\n",
    "level='mod'\n",
    "lead_int=3\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=0\n",
    "season_str='JJAS'\n",
    "level='mod'\n",
    "lead_int=4\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=0\n",
    "season_str='JJAS'\n",
    "level='mod'\n",
    "lead_int=5\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "#sev##################\n",
    "\n",
    "region_id=0\n",
    "season_str='JJAS'\n",
    "level='sev'\n",
    "lead_int=0\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "\n",
    "region_id=0\n",
    "season_str='JJAS'\n",
    "level='sev'\n",
    "lead_int=2\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=0\n",
    "season_str='JJAS'\n",
    "level='sev'\n",
    "lead_int=3\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=0\n",
    "season_str='JJAS'\n",
    "level='sev'\n",
    "lead_int=4\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=0\n",
    "season_str='JJAS'\n",
    "level='sev'\n",
    "lead_int=5\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "#ext##################\n",
    "region_id=0\n",
    "season_str='JJAS'\n",
    "level='ext'\n",
    "lead_int=0\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "\n",
    "region_id=0\n",
    "season_str='JJAS'\n",
    "level='ext'\n",
    "lead_int=2\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=0\n",
    "season_str='JJAS'\n",
    "level='ext'\n",
    "lead_int=3\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=0\n",
    "season_str='JJAS'\n",
    "level='ext'\n",
    "lead_int=4\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)\n",
    "\n",
    "region_id=0\n",
    "season_str='JJAS'\n",
    "level='ext'\n",
    "lead_int=5\n",
    "obs_data, ens_data=make_obs_fct_dataset(region_id,season_str,lead_int)\n",
    "score_gen(obs_data,ens_data,lead_int,region_id,season_str,level)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
